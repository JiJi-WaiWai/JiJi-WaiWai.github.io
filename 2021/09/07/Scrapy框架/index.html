<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.css" integrity="sha256-no0c5ccDODBwp+9hSmV5VvPpKwHCpbVzXHexIkupM6U=" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.js" integrity="sha256-a5YRB27CcBwBFcT5EF/f3E4vzIqyHrSR878nseNYw64=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"jiji-waiwai.github.io","root":"/","images":"/images","scheme":"Gemini","version":"8.6.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"livere","storage":true,"lazyload":false,"nav":null,"activeClass":"livere"},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>
<meta name="description" content="写一个爬虫，需要做很多事情。比如：发送网络请求、数据解析、数据存储、反反爬虫机制（更换IP代理、设置请求头等）、异步请求等。这些工作如果每次都要写的话，比较浪费时间。因此Scrapy把一些基础东西封装好了，提高爬取效率和开发效率。">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy框架">
<meta property="og:url" content="https://jiji-waiwai.github.io/2021/09/07/Scrapy%E6%A1%86%E6%9E%B6/index.html">
<meta property="og:site_name" content="De&#39;s Blog">
<meta property="og:description" content="写一个爬虫，需要做很多事情。比如：发送网络请求、数据解析、数据存储、反反爬虫机制（更换IP代理、设置请求头等）、异步请求等。这些工作如果每次都要写的话，比较浪费时间。因此Scrapy把一些基础东西封装好了，提高爬取效率和开发效率。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://z3.ax1x.com/2021/09/07/hIxYRI.png">
<meta property="og:image" content="https://z3.ax1x.com/2021/10/10/5At6je.png">
<meta property="article:published_time" content="2021-09-07T15:37:58.000Z">
<meta property="article:modified_time" content="2021-11-07T07:48:32.426Z">
<meta property="article:author" content="唧唧歪歪">
<meta property="article:tag" content="python">
<meta property="article:tag" content="爬虫">
<meta property="article:tag" content="scrapy">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://z3.ax1x.com/2021/09/07/hIxYRI.png">


<link rel="canonical" href="https://jiji-waiwai.github.io/2021/09/07/Scrapy%E6%A1%86%E6%9E%B6/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://jiji-waiwai.github.io/2021/09/07/Scrapy%E6%A1%86%E6%9E%B6/","path":"2021/09/07/Scrapy框架/","title":"Scrapy框架"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Scrapy框架 | De's Blog</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>
  <a target="_blank" rel="noopener" href="https://GitHub.com/JiJi-WaiWai" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#FD6C6C; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">De's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Scrapy%E6%A1%86%E6%9E%B6%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D%EF%BC%9A"><span class="nav-number">1.</span> <span class="nav-text">Scrapy框架架构介绍：</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE%EF%BC%9A"><span class="nav-number">2.</span> <span class="nav-text">创建项目：</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%9E%E6%88%98-%E5%8F%A4%E8%AF%97%E6%96%87%E7%BD%91%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98%EF%BC%9A"><span class="nav-number">3.</span> <span class="nav-text">实战-古诗文网爬虫实战：</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%84%E6%96%87%E4%BB%B6%E4%BB%A3%E7%A0%81%E5%A6%82%E4%B8%8B%EF%BC%9A"><span class="nav-number">3.1.</span> <span class="nav-text">各文件代码如下：</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CrawlSpider%E7%88%AC%E8%99%AB"><span class="nav-number">4.</span> <span class="nav-text">CrawlSpider爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89url%E8%A7%84%E5%88%99"><span class="nav-number">4.1.</span> <span class="nav-text">定义url规则</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Rule%E8%A7%84%E5%88%99%E7%B1%BB%EF%BC%9A"><span class="nav-number">4.1.1.</span> <span class="nav-text">Rule规则类：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LinkExtractors%E9%93%BE%E6%8E%A5%E6%8F%90%E5%8F%96%E5%99%A8%EF%BC%9A"><span class="nav-number">4.1.2.</span> <span class="nav-text">LinkExtractors链接提取器：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E6%88%98-%E7%8C%8E%E4%BA%91%E7%BD%91%E7%88%AC%E8%99%AB%EF%BC%9A"><span class="nav-number">4.2.</span> <span class="nav-text">实战-猎云网爬虫：</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#scrapy%E4%B8%8B%E8%BD%BD%E5%9B%BE%E7%89%87"><span class="nav-number">5.</span> <span class="nav-text">scrapy下载图片</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E6%88%98-zcool%E7%BD%91%E7%AB%99%E4%B8%8B%E8%BD%BD%E5%9B%BE%E7%89%87"><span class="nav-number">5.1.</span> <span class="nav-text">实战-zcool网站下载图片</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BD%E5%99%A8%E4%B8%AD%E9%97%B4%E4%BB%B6%EF%BC%9A"><span class="nav-number">6.</span> <span class="nav-text">下载器中间件：</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#process-request-self-request-spider"><span class="nav-number">6.1.</span> <span class="nav-text">process_request(self, request, spider)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#process-response-self-request-response-spider"><span class="nav-number">6.2.</span> <span class="nav-text">process_response(self, request, response, spider)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%9B%B4%E6%8D%A2%E8%AF%B7%E6%B1%82%E5%A4%B4User-Agent"><span class="nav-number">6.3.</span> <span class="nav-text">随机更换请求头User-Agent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%BE%E7%BD%AEIP%E4%BB%A3%E7%90%86%EF%BC%9A"><span class="nav-number">6.4.</span> <span class="nav-text">设置IP代理：</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%9E%E6%88%98-%E6%9B%B4%E6%8D%A2%E4%BB%A3%E7%90%86%E7%88%AC%E5%8F%96%E7%8C%8E%E8%81%98%E7%BD%91%EF%BC%9A"><span class="nav-number">7.</span> <span class="nav-text">实战-更换代理爬取猎聘网：</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#scrapy%E9%9B%86%E6%88%90selenium%E7%88%AC%E5%8F%96%E7%BD%91%E9%A1%B5"><span class="nav-number">8.</span> <span class="nav-text">scrapy集成selenium爬取网页</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="唧唧歪歪"
      src="/images/1.jpg">
  <p class="site-author-name" itemprop="name">唧唧歪歪</p>
  <div class="site-description" itemprop="description">踏上新征程----go！！！</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/JiJi-WaiWai" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;JiJi-WaiWai" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:1753645532@qq.com" title="E-Mail → mailto:1753645532@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://tool.gljlw.com/qq/?qq=1753645532" title="http:&#x2F;&#x2F;tool.gljlw.com&#x2F;qq&#x2F;?qq&#x3D;1753645532" rel="noopener" target="_blank">加qq</a>
        </li>
    </ul>
  </div>

          </div>
        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiji-waiwai.github.io/2021/09/07/Scrapy%E6%A1%86%E6%9E%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/1.jpg">
      <meta itemprop="name" content="唧唧歪歪">
      <meta itemprop="description" content="踏上新征程----go！！！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="De's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Scrapy框架
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-09-07 23:37:58" itemprop="dateCreated datePublished" datetime="2021-09-07T23:37:58+08:00">2021-09-07</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-11-07 15:48:32" itemprop="dateModified" datetime="2021-11-07T15:48:32+08:00">2021-11-07</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>写一个爬虫，需要做很多事情。比如：发送网络请求、数据解析、数据存储、反反爬虫机制（更换IP代理、设置请求头等）、异步请求等。这些工作如果每次都要写的话，比较浪费时间。因此Scrapy把一些基础东西封装好了，提高爬取效率和开发效率。</p>
<span id="more"></span>



<p>Scrapy官方文档：<a target="_blank" rel="noopener" href="http://doc.scrapy.org/en/latest">http://doc.scrapy.org/en/latest</a></p>
<p>Scrapy中文文档：<a target="_blank" rel="noopener" href="http://scrapy-chs.readthedocs.io/zh_CN/latest/index.html">http://scrapy-chs.readthedocs.io/zh_CN/latest/index.html</a></p>
<p><strong>安装</strong>:</p>
<p>pip install scrapy</p>
<p>可能会出现的问题：</p>
<p>1,在windos系统下，提示错误ModuleNotFoundError:No module nmed ‘win32api’，那么先使用命令安装：pip install pypiwin32</p>
<p>2,如果安装时提示twisted安装有问题，那么先到这个网站下载twisted的whl文件：<a target="_blank" rel="noopener" href="https://www.lfd.uci.edu/~gohlke/pythonlibs/%EF%BC%8C%E6%A0%B9%E6%8D%AE%E8%87%AA%E5%B7%B1%E7%9A%84python%E7%89%88%E6%9C%AC%E4%B8%8B%E8%BD%BD%E3%80%82%E4%B8%8B%E8%BD%BD%E5%AE%8C%E5%90%8E%EF%BC%8C%E5%86%8D%E4%BD%BF%E7%94%A8pip">https://www.lfd.uci.edu/~gohlke/pythonlibs/，根据自己的python版本下载。下载完后，再使用pip</a> install xxx.whl安装刚刚下载的文件。</p>
<h1 id="Scrapy框架架构介绍："><a href="#Scrapy框架架构介绍：" class="headerlink" title="Scrapy框架架构介绍："></a>Scrapy框架架构介绍：</h1><ol>
<li>Scrapy Engine（引擎）：Scrapy框架的核心部分。负责在Spider和ItemPipeline、Downloader、Scheduler中间通信、传递数据等。</li>
<li>Spider（爬虫）：发送需要爬取的链接给引擎，最后引擎把其他模块请求回来的数据再发送给爬虫，爬虫就去解析想要的数据。这个部分是我们开发者自己写的，因为要爬取哪些链接，页面中的哪些数据是需要的，都是由程序员自己决定。</li>
<li>Scheduler（调度器）：负责接收引擎发送过来的请求，并按照一定的方式进行排列和整理，负责调度请求的顺序等。</li>
<li>Downloader（下载器）：负责接收引擎传过来的下载请求，然后去网络上下载对应的数据再交还给引擎。</li>
<li>Item Pipeline（管道）：负责将Spider（爬虫）传递过来的数据进行保存。具体保存在哪里，应该看开发者自己的需求。</li>
<li>Downloader Middlewares（下载中间件）：可以扩展下载器和引擎之间通信功能的中间件。</li>
<li>Spider Middlewares（Spider中间件）：可以扩展引擎和爬虫之间通信功能的中间件。</li>
</ol>
<p><strong>工作流程：</strong></p>
<p><a target="_blank" rel="noopener" href="https://imgtu.com/i/hIxYRI"><img src="https://z3.ax1x.com/2021/09/07/hIxYRI.png" alt="hIxYRI.png"></a></p>
<h1 id="创建项目："><a href="#创建项目：" class="headerlink" title="创建项目："></a>创建项目：</h1><p>用cmd创建项目，先进入到项目存放的目录。</p>
<p>1，创建项目：<br>scrapy startproject [项目名称]</p>
<p>2，进入项目，然后创建爬虫：<br>scrapy genspider [爬虫名称] [爬虫的作用域名]</p>
<p><strong>目录结构介绍</strong>：</p>
<p>items.py：用来存放爬虫爬取下来的数据的模型</p>
<p>middlewares.py：用来存放各种中间件的文件</p>
<p>pipelines.py：用来将items的模型存储到本地磁盘中</p>
<p>settings.py：本爬虫的一些配置信息（比如请求头、多久发送一次请求、ip代理池等）</p>
<p>scrapy.cfg：项目的配置文件</p>
<p>spiders包：以后所有的爬虫，对是存放到这个里面</p>
<h1 id="实战-古诗文网爬虫实战："><a href="#实战-古诗文网爬虫实战：" class="headerlink" title="实战-古诗文网爬虫实战："></a>实战-古诗文网爬虫实战：</h1><p>先在settings.py文件中配置：</p>
<ul>
<li><p>将ROBOTSTXT_OBEY改为False，表示不遵守机器人协议</p>
</li>
<li><p>在DEFAULT_REQUEST_HEADERS里添加请求头信息</p>
</li>
<li><p>ITEM_PIPELINES设置pipelines.py的优先级</p>
</li>
</ul>
<p>在cmd中输入命令：scrapy crawl [爬虫名称]，运行指定爬虫。每次都cmd手动输入会很麻烦，所以在项目中新建一个py文件，内容如下。运行该文件就相当于在cmd中输入了相关命令</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#运行该文件，相当于运行爬虫</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> cmdline</span><br><span class="line"></span><br><span class="line">cmds = [<span class="string">&quot;scrapy&quot;</span>, <span class="string">&quot;crawl&quot;</span>, <span class="string">&quot;gsww_spider&quot;</span>]</span><br><span class="line">cmdline.execute(cmds)</span><br></pre></td></tr></table></figure>

<p>编写代码时，每次print打印时，会有大量其他不相关内容，因此分辨不清，所以自己定义了一个myprint方法，用myprint打印内容方便观察，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myprint</span>(<span class="params">self, value</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;=&quot;</span>*<span class="number">50</span>)</span><br><span class="line">    <span class="built_in">print</span>(value)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;=&quot;</span>*<span class="number">50</span>)</span><br></pre></td></tr></table></figure>

<h2 id="各文件代码如下："><a href="#各文件代码如下：" class="headerlink" title="各文件代码如下："></a>各文件代码如下：</h2><p>爬虫文件代码(gsww_spider)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> GswwItem  <span class="comment">#导入item模型</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GswwSpiderSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;gsww_spider&#x27;</span>  <span class="comment">#爬虫名称</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;gushiwen.cn&#x27;</span>]   <span class="comment">#爬虫的作用域名</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.gushiwen.cn/default_1.aspx&#x27;</span>]   <span class="comment">#爬虫爬取的初始url</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span>   <span class="comment">#主要内容在这里面编写</span></span><br><span class="line">        bList = response.xpath(<span class="string">&quot;//div[@class=&#x27;left&#x27;]/div[@class=&#x27;sons&#x27;]/div[@class=&#x27;cont&#x27;]//b&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> bList:</span><br><span class="line">            title = b.xpath(<span class="string">&quot;./text()&quot;</span>).get()   <span class="comment">#标题</span></span><br><span class="line">            cont = b.xpath(<span class="string">&quot;..&quot;</span>).xpath(<span class="string">&quot;..&quot;</span>).xpath(<span class="string">&quot;..&quot;</span>)</span><br><span class="line">            source = cont.xpath(<span class="string">&quot;./p[@class=&#x27;source&#x27;]/a/text()&quot;</span>).getall()</span><br><span class="line">            author = source[<span class="number">0</span>]   <span class="comment">#作者</span></span><br><span class="line">            dynasty = source[<span class="number">1</span>]  <span class="comment">#朝代</span></span><br><span class="line">            content = cont.xpath(<span class="string">&quot;./div[@class=&#x27;contson&#x27;]//text()&quot;</span>).getall()</span><br><span class="line">            content = <span class="string">&quot;&quot;</span>.join(content).strip()   <span class="comment">#内容</span></span><br><span class="line">            item = GswwItem(title=title, author=author, dynasty=dynasty, content=content)</span><br><span class="line">            <span class="keyword">yield</span> item    <span class="comment">#每次yield item，将item传入pipelines.py文件并调用,表示存储数据</span></span><br><span class="line">            </span><br><span class="line">        url = response.xpath(<span class="string">&quot;//a[@id=&#x27;amore&#x27;]/@href&quot;</span>).get()</span><br><span class="line">        <span class="comment">#response.urljoin(url)  域名+指定的url</span></span><br><span class="line">        <span class="keyword">if</span> url:</span><br><span class="line">            request = scrapy.Request(url)</span><br><span class="line">            <span class="keyword">yield</span> request   <span class="comment">#每次yield request，会重新调用所在的parse方法，表示发送请求</span></span><br></pre></td></tr></table></figure>

<p>items.py文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define here the models for your scraped items</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://docs.scrapy.org/en/latest/topics/items.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GswwItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    author = scrapy.Field()</span><br><span class="line">    dynasty = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br></pre></td></tr></table></figure>

<p>pipelines.py文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># useful for handling different item types with a single interface</span></span><br><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> ItemAdapter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GswwPipeline</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span>(<span class="params">self, spider</span>):</span>   <span class="comment">#爬虫开始时调用，用于打开文件</span></span><br><span class="line">        self.fp = <span class="built_in">open</span>(<span class="string">&quot;古诗文.csv&quot;</span>, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>, newline=<span class="string">&quot;&quot;</span>)</span><br><span class="line">        self.writer = csv.writer(self.fp)</span><br><span class="line">        self.writer.writerow([<span class="string">&quot;title&quot;</span>, <span class="string">&quot;author&quot;</span>, <span class="string">&quot;dynasty&quot;</span>, <span class="string">&quot;content&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span>  <span class="comment">#存储数据操作放在该方法中</span></span><br><span class="line">        item_dict = <span class="built_in">dict</span>(item)</span><br><span class="line">        title = item_dict[<span class="string">&quot;title&quot;</span>]</span><br><span class="line">        author = item_dict[<span class="string">&quot;author&quot;</span>]</span><br><span class="line">        dynasty = item_dict[<span class="string">&quot;dynasty&quot;</span>]</span><br><span class="line">        content = item_dict[<span class="string">&quot;content&quot;</span>]</span><br><span class="line">        self.writer.writerow([title, author, dynasty, content])</span><br><span class="line">        <span class="keyword">return</span> item   <span class="comment">#必须要</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self, spider</span>):</span>   <span class="comment">#爬虫结束时调用，用于关闭文件</span></span><br><span class="line">        self.fp.close()</span><br></pre></td></tr></table></figure>

<p><strong>注：</strong><br> .xpath()，返回的都是SelectorList对象<br>SelectorList.getall()，直接获取里面指定的值，是个列表<br>SelectorList.get()，直接获取里面指定的第一个的值</p>
<h1 id="CrawlSpider爬虫"><a href="#CrawlSpider爬虫" class="headerlink" title="CrawlSpider爬虫"></a>CrawlSpider爬虫</h1><p>之前使用的spider，请求完url后，想要继续请求url需要自己手动yield request。创建方法：scrapy genspider [爬虫名称] [爬虫的作用域名]。</p>
<p>CrawlSpider继承spider，但是有新的功能。可以自己定义url规则，当scrapy碰到满足规则的url时，自动去访问该url，而不需要手动yield request。创建方法：scrapy genspider -t crawl  [爬虫名称] [爬虫的作用域名]。</p>
<h2 id="定义url规则"><a href="#定义url规则" class="headerlink" title="定义url规则"></a>定义url规则</h2><p>在rules中写规则。</p>
<h3 id="Rule规则类："><a href="#Rule规则类：" class="headerlink" title="Rule规则类："></a>Rule规则类：</h3><p>Rule(link_extractor, callback=None, follow=None, process_links=None)</p>
<p>主要参数讲解：</p>
<p>1，link_extractor： 一个LinkExtractor对象，用于定义爬取规则。</p>
<p>2，callback：满足规则的url，需要执行哪个回调函数。</p>
<p>3，follow：在访问的url中，如果还有符合规则的url，需不需要跟进（即继续访问）</p>
<p>4，process_links：从link_extractor中获取到链接后会传递给这个函数，用来过滤不需要访问的链接。</p>
<h3 id="LinkExtractors链接提取器："><a href="#LinkExtractors链接提取器：" class="headerlink" title="LinkExtractors链接提取器："></a>LinkExtractors链接提取器：</h3><p>主要参数讲解：</p>
<p>allow：允许的url，所有满足该正则表达式的url都会被提取。</p>
<p>deny：禁止的url，所有满足该正则表达式的url都不会被提取。</p>
<p>allow_domains：允许的域名，只有在该域名下的url才会被提取。</p>
<p>deny_domains：禁止的域名，在该域名下的url都不会被提取。</p>
<p>restrict_xpaths：严格的xpath。和allow共同过滤链接。指定某个xpath规定的范围内的url。</p>
<h2 id="实战-猎云网爬虫："><a href="#实战-猎云网爬虫：" class="headerlink" title="实战-猎云网爬虫："></a>实战-猎云网爬虫：</h2><p>要求：使用CrawlSpider爬虫爬取，然后异步保存到mysql数据库</p>
<p>网站：<a target="_blank" rel="noopener" href="https://www.lieyunwang.com/">https://www.lieyunwang.com/</a></p>
<p>步骤：</p>
<p>创建项目，创建CrawlSpider，在settings.py文件中更改配置信息</p>
<ul>
<li>将ROBOTSTXT_OBEY改为False，表示不遵守机器人协议</li>
<li>在DEFAULT_REQUEST_HEADERS里添加请求头信息</li>
<li>ITEM_PIPELINES设置pipelines.py的优先级</li>
</ul>
<p>爬虫文件(gsww_spider)代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> LywItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LywSpiderSpider</span>(<span class="params">CrawlSpider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;lyw_spider&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;lieyunwang.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.lieyunwang.com/&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">	<span class="comment">#定义的规则</span></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;latest/p\d+\.html&#x27;</span>), follow=<span class="literal">True</span>),</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;archives/\d+&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">False</span>),</span><br><span class="line">        <span class="comment"># Rule(LinkExtractor(allow=r&#x27;latest/p1\.html&#x27;), follow=True),</span></span><br><span class="line">        <span class="comment"># Rule(LinkExtractor(allow=r&#x27;archives/477777&#x27;), callback=&#x27;parse_item&#x27;, follow=False),</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">myprint</span>(<span class="params">self, value</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;=&#x27;</span>*<span class="number">30</span>)</span><br><span class="line">        <span class="built_in">print</span>(value)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;=&#x27;</span>*<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        title = response.xpath(<span class="string">&quot;//h1[@class=&#x27;lyw-article-title-inner&#x27;]/text()&quot;</span>).getall()</span><br><span class="line">        title = <span class="string">&#x27;&#x27;</span>.join(title).strip()</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            date = response.xpath(<span class="string">&quot;//div[@id=&#x27;main-text-id&#x27;]/p[1]//text()&quot;</span>).getall()[<span class="number">2</span>]</span><br><span class="line">            date = re.search(<span class="string">&#x27;】(.+)报道&#x27;</span>, date).group(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            date = <span class="string">&quot;NULL&quot;</span></span><br><span class="line">        content = response.xpath(<span class="string">&quot;//div[@id=&#x27;main-text-id&#x27;]//text()&quot;</span>).getall()</span><br><span class="line">        content = <span class="string">&#x27;&#x27;</span>.join(content).strip()</span><br><span class="line">        origin = response.url</span><br><span class="line"></span><br><span class="line">        item = LywItem(title=title, date=date, content=content, origin=origin)</span><br><span class="line">        <span class="keyword">return</span> item   <span class="comment">#这里可以不使用yield item.</span></span><br></pre></td></tr></table></figure>

<p>item.py 文件代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LywItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    date = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br><span class="line">    origin = scrapy.Field()</span><br></pre></td></tr></table></figure>

<p>1，正常保存到mysql数据库，没有使用异步</p>
<p>pipelines.py文件代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="comment">#正常不使用异步保存到mysql数据库</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LywPipeline</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span>(<span class="params">self, spider</span>):</span></span><br><span class="line">        self.con = pymysql.connect(host=<span class="string">&quot;127.0.0.1&quot;</span>, port=<span class="number">3306</span>, user=<span class="string">&quot;root&quot;</span>, password=<span class="string">&quot;root&quot;</span>, database=<span class="string">&quot;de&quot;</span>, charset=<span class="string">&quot;utf8&quot;</span>)</span><br><span class="line">        self.cursor = self.con.cursor()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line">        sql = <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        insert into lyw_info (title, the_date, content, origin) value (%s, %s, %s, %s)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        args = (item[<span class="string">&quot;title&quot;</span>], item[<span class="string">&quot;date&quot;</span>], item[<span class="string">&quot;content&quot;</span>], item[<span class="string">&quot;origin&quot;</span>])</span><br><span class="line">        self.cursor.execute(sql, args)</span><br><span class="line">        self.con.commit()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self, spider</span>):</span></span><br><span class="line">        self.cursor.close()</span><br><span class="line">        self.con.close()</span><br></pre></td></tr></table></figure>

<p>2，使用异步，保存到mysql数据库</p>
<p><strong>异步操作数据库的方法</strong>：</p>
<ul>
<li>使用<code>twisted.enterprise.adbapi</code>来创建连接池ConnectionPool。</li>
<li>连接池使用<code>runInteraction</code>函数来运行执行sql语句的函数。</li>
<li>执行sql语句的函数中，第一个非self参数是cursor对象，使用该对象的execute执行sql语句。</li>
</ul>
<p><strong>实际步骤</strong>：</p>
<p>先在settings.py文件中配置数据库信息</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">MYSQL_CONFIG = &#123;</span><br><span class="line">    <span class="string">&#x27;DRIVER&#x27;</span>: <span class="string">&#x27;pymysql&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;HOST&#x27;</span>: <span class="string">&#x27;localhost&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;PORT&#x27;</span>: <span class="number">3306</span>,</span><br><span class="line">    <span class="string">&#x27;USER&#x27;</span>: <span class="string">&#x27;root&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;PASSWORD&#x27;</span>: <span class="string">&#x27;root&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;DATABASE&#x27;</span>: <span class="string">&#x27;de&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;CHARSET&#x27;</span>: <span class="string">&#x27;utf8&#x27;</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后pipelines.py文件代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> twisted.enterprise <span class="keyword">import</span> adbapi</span><br><span class="line"></span><br><span class="line"><span class="comment">##异步保存到mysql数据库</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LywPipeline</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, mysql_config</span>):</span></span><br><span class="line">        <span class="comment"># 1,创建连接池，配置好数据库。</span></span><br><span class="line">        self.dbPoll = adbapi.ConnectionPool(</span><br><span class="line">            mysql_config[<span class="string">&quot;DRIVER&quot;</span>],</span><br><span class="line">            host=mysql_config[<span class="string">&#x27;HOST&#x27;</span>],</span><br><span class="line">            port=mysql_config[<span class="string">&#x27;PORT&#x27;</span>],</span><br><span class="line">            user=mysql_config[<span class="string">&quot;USER&quot;</span>],</span><br><span class="line">            password=mysql_config[<span class="string">&quot;PASSWORD&quot;</span>],</span><br><span class="line">            database=mysql_config[<span class="string">&#x27;DATABASE&#x27;</span>],</span><br><span class="line">            charset=mysql_config[<span class="string">&#x27;CHARSET&#x27;</span>]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="comment">#只要重写了该from_crawler类方法，创建对象的时候，会自动掉用该方法获取pipeline对象。</span></span><br><span class="line">    <span class="comment">#这里用来获取保存在settings.py中的数据库配置信息,mysql_config。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls,crawler</span>):</span></span><br><span class="line">        mysql_config = crawler.settings[<span class="string">&quot;MYSQL_CONFIG&quot;</span>]</span><br><span class="line">        <span class="keyword">return</span> cls(mysql_config)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line">        <span class="comment">#2，使用`runInteraction`函数来运行执行sql语句的函数，还可以传递参数给执行sql语句的函数，比如item</span></span><br><span class="line">        self.dbPoll.runInteraction(self.inset_item, item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inset_item</span>(<span class="params">self,cursor,item</span>):</span></span><br><span class="line">        <span class="comment">#3，该函数为执行sql语句的函数。第一个非self参数是cursor对象，item参数是`runInteraction`函数传递过来的。</span></span><br><span class="line">        sql = <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        insert into lyw_info (title, the_date, content, origin) value (%s,%s,%s,%s)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        args = (item[<span class="string">&#x27;title&#x27;</span>], item[<span class="string">&#x27;date&#x27;</span>], item[<span class="string">&#x27;content&#x27;</span>], item[<span class="string">&#x27;origin&#x27;</span>])</span><br><span class="line">        cursor.execute(sql, args)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self, spider</span>):</span></span><br><span class="line">        <span class="comment">#关闭连接池</span></span><br><span class="line">        self.dbPoll.close()</span><br></pre></td></tr></table></figure>



<p>出现的问题点：scrapy运行正常，pipeline管道经过测试是没有运行的。判断已经主要是并没有创建设置的IMAGES_STORE<br>提前指出解决的办法，主要是没有依赖处理图片的第三库pillow</p>
<p>pip install -i <a target="_blank" rel="noopener" href="https://pypi.doubanio.com/simple/">https://pypi.doubanio.com/simple/</a> –trusted-host pypi.doubanio.com pillow</p>
<h1 id="scrapy下载图片"><a href="#scrapy下载图片" class="headerlink" title="scrapy下载图片"></a>scrapy下载图片</h1><p><strong>流程：</strong></p>
<p>1，解析图片的url</p>
<p>2，定义item。item中必须要有image_urls和images两个字段。image_urls中存放图片的url。</p>
<p>3，在settings.py中，使用scrapy.pipelines.images.ImagesPipeline来作为数据保存的pipeline。</p>
<p>4，在settings.py中，设置IMAGE_STORE来定义图片下载的路径。</p>
<p>此时，只操作以上步骤的话。指定的IMAGE_STORE路径中会自动生成full文件夹，所有图片保存在里面。</p>
<p>5，如果想要有更复杂的图片保存路径的需求。先在settings.py中，关掉第2步的‘scrapy.pipelines.images.ImagesPipeline’，打开‘zcool.pipelines.ZcoolPipeline’。然后在pipelines.py中重写ImagePipiline类的file_path方法。该方法用来返回每个图片的保存路径。</p>
<p>6，但是file_path方法中没有item对象。如果想要item对象的话，我们还需要重写ImagePipline类的get_media_requests方法,来把item绑定到request上。</p>
<h2 id="实战-zcool网站下载图片"><a href="#实战-zcool网站下载图片" class="headerlink" title="实战-zcool网站下载图片"></a>实战-zcool网站下载图片</h2><p>网站：<a target="_blank" rel="noopener" href="https://www.zcool.com.cn/">https://www.zcool.com.cn/</a></p>
<p>把相同作品的图片下载到用一个文件夹，文件夹名就为作品名称</p>
<p><strong>实际步骤如下：</strong>（对照上面的流程）</p>
<p>创建scrapy项目，创建crawlSpider，在settings.py中更改配置，如不遵守机器人协议，请求头</p>
<p>流程一，zcool_spider.py文件代码：(访问网站，解析图片url)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> ZcoolItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZcoolSpiderSpider</span>(<span class="params">CrawlSpider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;zcool_spider&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;zcool.com.cn&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.zcool.com.cn/home?p=1#tab_anchor&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;home\?p=\d+#tab_anchor&#x27;</span>), follow=<span class="literal">True</span>),</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;work/.+=\.html&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">False</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">myprint</span>(<span class="params">self, value</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;=&quot;</span>*<span class="number">40</span>)</span><br><span class="line">        <span class="built_in">print</span>(value)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;=&quot;</span> * <span class="number">40</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        title = response.xpath(<span class="string">&quot;//div[@class=&#x27;details-contitle-box&#x27;]/h2/text()&quot;</span>).getall()</span><br><span class="line">        title = <span class="string">&#x27;&#x27;</span>.join(title).strip()</span><br><span class="line">        image_urls = response.xpath(<span class="string">&quot;//div[@class=&#x27;reveal-work-wrap js-sdata-box text-center&#x27;]//img/@src&quot;</span>).getall()</span><br><span class="line"></span><br><span class="line">        item = ZcoolItem(title=title, image_urls=image_urls)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<p>流程二，item.py文件代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZcoolItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    <span class="comment">#image_urls 和 images必须要有。</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    image_urls = scrapy.Field()</span><br><span class="line">    images = scrapy.Field()</span><br></pre></td></tr></table></figure>

<p>流程三四，配置settings.py文件：<br>1，设置ITEM_PIPELINES，如果图片放在一起就开启scrapy.pipelines.images.ImagesPipeline；如果有其他存放路径需求，就开启zcool.pipelines.ZcoolPipeline，并接着下一步在pipelines.py文件中重写方法</p>
<p>2，添加IMAGES_STORE，是图片的保存目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   &#x27;zcool.pipelines.ZcoolPipeline&#x27;: 300,</span><br><span class="line">   #  &#x27;scrapy.pipelines.images.ImagesPipeline&#x27;: 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">IMAGES_STORE = os.path.join(os.path.dirname(os.path.dirname(__file__)),&#x27;images&#x27;)</span><br></pre></td></tr></table></figure>

<p>流程五六，在pipelines.py文件中，重写ImagePipiline类的file_path方法和get_media_requests方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> ItemAdapter</span><br><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"><span class="keyword">from</span> zcool <span class="keyword">import</span> settings</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZcoolPipeline</span>(<span class="params">ImagesPipeline</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span>(<span class="params">self, item, info</span>):</span></span><br><span class="line">        <span class="comment">#重写该方法的目的是，把item绑定在request上面，然后下面的file_path方法可以访问到item</span></span><br><span class="line">        media_requests = <span class="built_in">super</span>(ZcoolPipeline, self).get_media_requests(item, info)</span><br><span class="line">        <span class="keyword">for</span> media_request <span class="keyword">in</span> media_requests:</span><br><span class="line">            media_request.item = item</span><br><span class="line">        <span class="keyword">return</span> media_requests</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">file_path</span>(<span class="params">self, request, response=<span class="literal">None</span>, info=<span class="literal">None</span></span>):</span></span><br><span class="line">        orige_path = <span class="built_in">super</span>(ZcoolPipeline, self).file_path(request, response, info)</span><br><span class="line"></span><br><span class="line">        title = request.item[<span class="string">&quot;title&quot;</span>]</span><br><span class="line">        title = re.sub(<span class="string">r&#x27;[/\\:\*\?&quot;&lt;&gt;\|]&#x27;</span>,<span class="string">&quot;_&quot;</span>,title)</span><br><span class="line">        image_path = os.path.join(settings.IMAGES_STORE,title)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(image_path):</span><br><span class="line">            os.mkdir(image_path)</span><br><span class="line">        image_name = orige_path.replace(<span class="string">&quot;full/&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">		<span class="comment">#返回值就是每个图片的保存路径</span></span><br><span class="line">        <span class="keyword">return</span> os.path.join(image_path, image_name)</span><br></pre></td></tr></table></figure>

<p>注意：创建文件夹时，名称不可用包含九种特殊符号。<br>我们的处理方式：<code>title = re.sub(r&#39;[/\\:\*\?&quot;&lt;&gt;\|]&#39;,&quot;_&quot;,title)</code></p>
<h1 id="下载器中间件："><a href="#下载器中间件：" class="headerlink" title="下载器中间件："></a>下载器中间件：</h1><p>Downloader Middlewares（下载中间件）是：引擎和下载器通信的中间件。在这个中间件里，可以设置ip代理、更换请求头等。主要有两个方法：</p>
<p>process_request(self, request, spider)：发送请求前调用</p>
<p>process_response(self, request, response, spider)：数据下载到引擎前调用</p>
<p><a target="_blank" rel="noopener" href="https://imgtu.com/i/5At6je"><img src="https://z3.ax1x.com/2021/10/10/5At6je.png" alt="5At6je.png"></a></p>
<h2 id="process-request-self-request-spider"><a href="#process-request-self-request-spider" class="headerlink" title="process_request(self, request, spider)"></a>process_request(self, request, spider)</h2><p>下载器发送请求前调用，这里可以设置IP代理、请求头等</p>
<p>返回值：</p>
<p>1，None：scrapy继续处理request，直到下载器被调用。</p>
<p>2，Request对象：不再使用之前的request对象，而是使用新的request对象。</p>
<p>3，Response对象：scrapy不再调用process_request方法，直接返回response，调用已激活的中间件的process_response方法。</p>
<p>4，如果方法抛出异常，会调用process_exception方法。</p>
<h2 id="process-response-self-request-response-spider"><a href="#process-response-self-request-response-spider" class="headerlink" title="process_response(self, request, response, spider)"></a>process_response(self, request, response, spider)</h2><p>下载器下载的数据到引擎中间会执行的方法。</p>
<p>返回值：</p>
<p>1，Response对象：会将这个新的response对象返回给其他中间件，直到引擎。</p>
<p>2，Request对象：下载器被切断，返回的request会重新被下载器调度下载。</p>
<p>3，如果抛出异常，会调用requst的errback方法。</p>
<h2 id="随机更换请求头User-Agent"><a href="#随机更换请求头User-Agent" class="headerlink" title="随机更换请求头User-Agent"></a>随机更换请求头User-Agent</h2><p>介绍fake_useragent包：</p>
<p><strong>fake_useragent</strong>模块用来伪造User-Agent的。下载：<code>pip install fake_useragent</code></p>
<p>用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line">ua = UserAgent()</span><br><span class="line">ua.ie       //ie浏览器的user-agent</span><br><span class="line">ua.chrome	//谷歌浏览器的user-agent</span><br><span class="line">ua.firefox  //火狐浏览器的user-agent</span><br><span class="line">ur.random   //任意浏览器的user-agent</span><br></pre></td></tr></table></figure>

<p>如果使用时报错：fake_useragent.errors.FakeUserAgentError: Maximum amount of retries reached。解决方法：</p>
<p>1，找到python》Lib》site-packages》fake_useragent》settings.py</p>
<p>2，打开后，把16行附近的’CACHE_SERVER’属性中的’https‘改成’http‘</p>
<p>3，然后运行程序，依旧会报错，但是出现了一个结果。然后再次运行程序，运行正常。</p>
<p><strong>更换请求头User-Agent步骤：</strong></p>
<p>爬虫文件代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UseragentSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;userAgent&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;httpbin.org&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://httpbin.org/user-agent&#x27;</span>]   <span class="comment">#该网址直接返回请求的user-agent</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;==================&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(response.text)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;==================&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(self.start_urls[<span class="number">0</span>], dont_filter=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>middlewares.py文件代码：(重点)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserAgentDownloadmiddleware</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">        ua = UserAgent()</span><br><span class="line">        request.headers[<span class="string">&quot;User-Agent&quot;</span>] = ua.random</span><br></pre></td></tr></table></figure>

<p>开启这个中间件，在settings.py中设置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">   <span class="string">&#x27;downloadmiddleware.middlewares.UserAgentDownloadmiddleware&#x27;</span>: <span class="number">543</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="设置IP代理："><a href="#设置IP代理：" class="headerlink" title="设置IP代理："></a>设置IP代理：</h2><p>芝麻代理：<a target="_blank" rel="noopener" href="http://www.zhimaruanjian.com/">http://www.zhimaruanjian.com</a></p>
<p>流程：</p>
<p>爬虫文件代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IpproxySpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;ipProxy&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;httpbin.org&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://httpbin.org/ip&#x27;</span>]  //该网址直接返回请求的ip</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;==================&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(response.text)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;==================&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(self.start_urls[<span class="number">0</span>], dont_filter=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>middlewares.py文件代码：（重点）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IpProxyDownloadmiddleware</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    proxies = [   //购买的代理</span><br><span class="line">        &#123;<span class="string">&quot;ip&quot;</span>:<span class="string">&quot;223.242.9.160&quot;</span>,<span class="string">&quot;port&quot;</span>:<span class="number">4245</span>,<span class="string">&quot;expire_time&quot;</span>:<span class="string">&quot;2021-10-10 13:40:46&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;ip&quot;</span>:<span class="string">&quot;124.94.187.11&quot;</span>,<span class="string">&quot;port&quot;</span>:<span class="number">4224</span>,<span class="string">&quot;expire_time&quot;</span>:<span class="string">&quot;2021-10-10 13:40:46&quot;</span>&#125;,</span><br><span class="line">               ]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">        proxy = random.choice(self.proxies)   //随机选择一个代理</span><br><span class="line">        <span class="comment"># 格式  http://223.242.9.160:4245</span></span><br><span class="line">        proxy_url = <span class="string">&quot;http://&quot;</span> + proxy[<span class="string">&quot;ip&quot;</span>] + <span class="string">&quot;:&quot;</span> + <span class="built_in">str</span>(proxy[<span class="string">&quot;port&quot;</span>])</span><br><span class="line">        request.meta[<span class="string">&quot;proxy&quot;</span>] = proxy_url</span><br></pre></td></tr></table></figure>

<p>开启这个中间件，在settings.py中设置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">   <span class="string">&#x27;downloadmiddleware.middlewares.IpProxyDownloadmiddleware&#x27;</span>: <span class="number">200</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>开放代理池：上面这种购买的代理ip属于开放代理池，就是一下买多个代理ip，这些ip随便使用，时间比较短。</p>
<p>独享代理：只买一个代理，这个代理ip时间会比较长，而且还有密码。</p>
<p>独享代理的使用例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IpProxyDownloadmiddleware</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">        proxy = <span class="string">&#x27;121.99.4.124:16816&#x27;</span>  //代理ip</span><br><span class="line">        pwd = <span class="string">&quot;156486:fjkdf4s&quot;</span>		  //密码</span><br><span class="line">        request.meta[<span class="string">&quot;proxy&quot;</span>] = proxy</span><br><span class="line">        <span class="comment">#密码要base64加密</span></span><br><span class="line">        pwd_b64 = base64.b64encode(pwd.encode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">        <span class="comment">#代理授权</span></span><br><span class="line">        request.headers[<span class="string">&#x27;Proxy-Authorization&#x27;</span>] = <span class="string">&#x27;Basic &#x27;</span> + paw_b64.decode(<span class="string">&quot;utf-8&quot;</span>)</span><br></pre></td></tr></table></figure>

<h1 id="实战-更换代理爬取猎聘网："><a href="#实战-更换代理爬取猎聘网：" class="headerlink" title="实战-更换代理爬取猎聘网："></a>实战-更换代理爬取猎聘网：</h1><p>网址：<a target="_blank" rel="noopener" href="https://www.liepin.com/">https://www.liepin.com/</a></p>
<p>要求：搜索python，把有关python的职位信息爬取下来，爬取时需要自动更换代理。</p>
<p>爬虫文件代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> LiepinItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LiepinSpiderSpider</span>(<span class="params">CrawlSpider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;liepin_spider&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;liepin.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.liepin.com/zhaopin/?headId=5299daf02f40ea2f23991628fa127774&amp;ckId=5299daf02f40ea2f23991628fa127774&amp;key=python&amp;currentPage=0&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;job/\d+\.shtml.*&#x27;</span>, restrict_xpaths=[<span class="string">&quot;//div[@class=&#x27;left-list-box&#x27;]/ul/li//div[@class=&#x27;job-detail-box&#x27;]/a[1]&quot;</span>]),callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">False</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">myprint</span>(<span class="params">self, value</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;=&quot;</span> * <span class="number">40</span>)</span><br><span class="line">        <span class="built_in">print</span>(value)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;=&quot;</span> * <span class="number">40</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        title = response.xpath(<span class="string">&quot;//span[@class=&#x27;name ellipsis-1&#x27;]/text()&quot;</span>).get()</span><br><span class="line">        company_1 = response.xpath(<span class="string">&quot;//div[@class=&#x27;title-box&#x27;]/span[1]//text()&quot;</span>).getall()</span><br><span class="line">        company_1 = <span class="string">&quot;&quot;</span>.join(company_1).strip()</span><br><span class="line">        company_2 = response.xpath(<span class="string">&quot;//div[@class=&#x27;title-box&#x27;]/span[2]//text()&quot;</span>).getall()</span><br><span class="line">        company_2 = <span class="string">&quot;&quot;</span>.join(company_2).strip()</span><br><span class="line">        company = company_1 + company_2</span><br><span class="line">        city = response.css(<span class="string">&quot;.job-properties&gt;span:nth-child(1)::text&quot;</span>).get()</span><br><span class="line">        experience = response.css(<span class="string">&quot;.job-properties&gt;span:nth-child(3)::text&quot;</span>).get()</span><br><span class="line">        edu = response.css(<span class="string">&quot;.job-properties&gt;span:nth-child(5)::text&quot;</span>).get()</span><br><span class="line">        salary = response.css(<span class="string">&quot;.salary::text&quot;</span>).get()</span><br><span class="line">        desc_list = response.css(<span class="string">&quot;.paragraph&gt;dd::text&quot;</span>).getall()</span><br><span class="line">        desc = <span class="string">&quot;&quot;</span>.join(desc_list).strip()</span><br><span class="line"></span><br><span class="line">        item = LiepinItem(title=title, company=company, city=city, experience=experience, edu=edu, salary=salary, desc=desc)</span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">        current_page = response.request.headers[<span class="string">&quot;Referer&quot;</span>].decode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">        page_num = <span class="built_in">int</span>( re.search(<span class="string">&quot;.*currentPage=(\d+)&quot;</span>,current_page).group(<span class="number">1</span>) )</span><br><span class="line">        next_page = <span class="string">&quot;https://www.liepin.com/zhaopin/?headId=5299daf02f40ea2f23991628fa127774&amp;ckId=5299daf02f40ea2f23991628fa127774&amp;key=python&amp;currentPage=&quot;</span> + <span class="built_in">str</span>( page_num+<span class="number">1</span> )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url=next_page)</span><br></pre></td></tr></table></figure>

<p>items.py文件代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LiepinItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    company = scrapy.Field()</span><br><span class="line">    city = scrapy.Field()</span><br><span class="line">    experience = scrapy.Field()</span><br><span class="line">    edu = scrapy.Field()</span><br><span class="line">    salary = scrapy.Field()</span><br><span class="line">    desc = scrapy.Field()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>pipelines.py文件代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LiepinPipeline</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span>(<span class="params">self, spider</span>):</span></span><br><span class="line">        self.fp = <span class="built_in">open</span>(<span class="string">&quot;zhaopin.txt&quot;</span>, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line">        self.fp.write( json.dumps(<span class="built_in">dict</span>(item), ensure_ascii=<span class="literal">False</span>) + <span class="string">&quot;\n&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self, spider</span>):</span></span><br><span class="line">        self.fp.close()</span><br></pre></td></tr></table></figure>

<p>middlewares.py文件代码：（重点，这里设置代理）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json, requests</span><br><span class="line"><span class="keyword">from</span> .models <span class="keyword">import</span> Proxy</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProxyDownloadMiddleware</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.current_proxy = <span class="literal">None</span></span><br><span class="line">        self.update_proxy()</span><br><span class="line">        self.lock = threading.Lock</span><br><span class="line">        <span class="comment">#创建一个线程，每60秒自动更换ip，或者ip被黑名单更换ip</span></span><br><span class="line">        th1 = threading.Thread(target=self.update_proxy_in_threading)</span><br><span class="line">        th1.start()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">        request.meta[<span class="string">&quot;proxy&quot;</span>] = self.current_proxy.proxy_url</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_response</span>(<span class="params">self, request, response, spider</span>):</span></span><br><span class="line">        <span class="keyword">if</span> response.status != <span class="number">200</span>:   <span class="comment">#状态码不等于200就当作ip被黑名单了</span></span><br><span class="line">            self.lock.acquire()</span><br><span class="line">            self.current_proxy.is_blacked = <span class="literal">True</span></span><br><span class="line">            self.lock.release()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;*^*^*(&amp;)&amp;(*&quot;</span> * <span class="number">50</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;IP代理被禁用了&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;*^*^*(&amp;)&amp;(*&quot;</span> * <span class="number">50</span>)</span><br><span class="line">            <span class="keyword">return</span> request</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_proxy</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment">#获取IP代理，自己去买</span></span><br><span class="line">        resp = requests.get(<span class="string">&quot;购买的直连ip网址&quot;</span>)</span><br><span class="line">        proxy_dict = json.loads(resp.text)</span><br><span class="line">        proxy = Proxy(proxy_dict)    <span class="comment">#自己定义的Proxy模型类，下面有</span></span><br><span class="line">        self.current_proxy = proxy</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;*^*^*(&amp;)&amp;(*&quot;</span> * <span class="number">50</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;更换了一次IP代理&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;*^*^*(&amp;)&amp;(*&quot;</span> * <span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_proxy_in_threading</span>(<span class="params">self</span>):</span></span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            time.sleep(<span class="number">10</span>)</span><br><span class="line">            <span class="keyword">if</span> count&gt;=<span class="number">6</span> <span class="keyword">or</span> self.current_proxy.is_blacked:</span><br><span class="line">                self.update_proxy()</span><br><span class="line">                count = <span class="number">0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                count += <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>Proxy模型类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Proxy</span>():</span></span><br><span class="line">    <span class="comment"># proxy的模型，方便管理</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, proxy_dict</span>):</span></span><br><span class="line">        data = proxy_dict[<span class="string">&quot;data&quot;</span>][<span class="number">0</span>]</span><br><span class="line">        self.proxy_url = <span class="string">&quot;https://&quot;</span> + data[<span class="string">&quot;ip&quot;</span>] + <span class="string">&quot;:&quot;</span> + <span class="built_in">str</span>(data[<span class="string">&quot;port&quot;</span>])</span><br><span class="line">        self.is_blacked = <span class="literal">False</span></span><br></pre></td></tr></table></figure>

<p>最后在settings.py中将该下载器中间件启用。</p>
<h1 id="scrapy集成selenium爬取网页"><a href="#scrapy集成selenium爬取网页" class="headerlink" title="scrapy集成selenium爬取网页"></a>scrapy集成selenium爬取网页</h1><p>在scrapy爬取网址时，有时因为需要加载ajax或者需要点击某个按钮后，需要的数据才能完整展示出来，可以在scrapy中写selenium解决这个问题。</p>
<p><strong>流程：</strong>下载器中间件的process_request(self, request, spider)方法，发送请求时调用，即发送请求前截拦，然后执行该方法。所以在该方法内，用selenium发送请求，处理，再封装成response对象返回。</p>
<p>1，截拦原始请求。<br>2，用selenium请求。<br>3，将selenium请求的数据封装成response对象并返回。</p>
<p><em><strong>案例：</strong></em></p>
<p>爬取简书网站：<a target="_blank" rel="noopener" href="http://jianshu.com/%EF%BC%8C%E7%AE%80%E4%B9%A6%E5%8F%91%E8%A1%A8%E7%9A%84%E6%96%87%E7%AB%A0%E4%B8%8B%E9%9D%A2%E6%9C%89%E6%98%BE%E7%A4%BA%E8%A2%AB%E6%9F%90%E6%9F%90%E4%B8%93%E9%A2%98%E6%94%B6%E5%85%A5%EF%BC%8C%E5%A6%82%E6%9E%9C%E4%B8%93%E9%A2%98%E5%A4%9A%E7%9A%84%E8%AF%9D%EF%BC%8C%E5%8F%AA%E4%BC%9A%E6%98%BE%E7%A4%BA%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%8C%E9%9C%80%E8%A6%81%E7%82%B9%E5%87%BB%E6%8C%89%E9%92%AE%E2%80%9C%E5%B1%95%E5%BC%80%E6%9B%B4%E5%A4%9A%E2%80%9D%E6%89%8D%E8%83%BD%E6%98%BE%E7%A4%BA%E5%85%A8%E9%83%A8%E3%80%82">http://jianshu.com/，简书发表的文章下面有显示被某某专题收入，如果专题多的话，只会显示一部分，需要点击按钮“展开更多”才能显示全部。</a></p>
<p>爬取简书，然后用selenium执行点击“展开更多”按钮后，把数据返回。</p>
<p><strong>代码如下：</strong></p>
<p>爬虫文件代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JianshuSpiderSpider</span>(<span class="params">CrawlSpider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;jianshu_spider&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;jianshu.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://jianshu.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;p/[0-9a-z]&#123;12&#125;&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">True</span>),</span><br><span class="line">        <span class="comment"># Rule(LinkExtractor(allow=r&#x27;p/dbc69681575f&#x27;), callback=&#x27;parse_item&#x27;, follow=True),</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">myprint</span>(<span class="params">self, value</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">40</span>)</span><br><span class="line">        <span class="built_in">print</span>(value)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">40</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        zhuantis = response.xpath(<span class="string">&quot;//div[@class=&#x27;_21bLU4 _3kbg6I&#x27;]/div[1]/div[1]/section[position()=3]/div[1]//text()&quot;</span>).getall()</span><br><span class="line">        self.myprint(zhuantis)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>middlewares.py文件：（<strong>重点</strong>）（记得在settings.py中开启）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> scrapy.http.response.html <span class="keyword">import</span> HtmlResponse   <span class="comment">#封装response对象用的。</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JianshuDownloaderMiddleware</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.driver_path = <span class="string">&quot;D:/webDriver/chromedriver_win32_95.exe&quot;</span></span><br><span class="line">        self.driver = webdriver.Chrome(executable_path=self.driver_path)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">        <span class="comment">#1,只要写了该方法就相当于截拦了原始请求</span></span><br><span class="line">        <span class="comment">#2，用selenium请求，并处理，点击“展开更多”按钮</span></span><br><span class="line">        <span class="keyword">if</span> re.match(<span class="string">r&quot;.*?\.com/p/[0-9a-z]&#123;12&#125;&quot;</span>, request.url):</span><br><span class="line">            self.driver.get(request.url)</span><br><span class="line"></span><br><span class="line">            btn_path = <span class="string">&quot;//div[@class=&#x27;_21bLU4 _3kbg6I&#x27;]/div[1]/div[1]/section[position()=3]/div[1]/div[1]&quot;</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                WebDriverWait(self.driver, <span class="number">10</span>).until(</span><br><span class="line">                    EC.element_to_be_clickable((By.XPATH, btn_path))</span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                response = HtmlResponse(request.url, body=self.driver.page_source, request=request, encoding=<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">                <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    btn = self.driver.find_element_by_xpath(btn_path)</span><br><span class="line">                    self.driver.execute_script(<span class="string">&quot;arguments[0].click()&quot;</span>, btn)</span><br><span class="line">                <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="comment">#3，最后把selenium访问的网页数据，封装成response对象并返回</span></span><br><span class="line">            response = HtmlResponse(request.url, body=self.driver.page_source, request=request, encoding=<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> response    <span class="comment">#这个返回的response给爬虫文件中的response对象。</span></span><br></pre></td></tr></table></figure>











































    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>感谢老铁的支持！</div>
  <button>
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechatpay.jpg" alt="唧唧歪歪 微信">
        <span>微信</span>
      </div>
      <div>
        <img src="/images/alipay.jpg" alt="唧唧歪歪 支付宝">
        <span>支付宝</span>
      </div>

  </div>
</div>

          <div class="post-tags">
              <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i> python</a>
              <a href="/tags/%E7%88%AC%E8%99%AB/" rel="tag"><i class="fa fa-tag"></i> 爬虫</a>
              <a href="/tags/scrapy/" rel="tag"><i class="fa fa-tag"></i> scrapy</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/09/07/%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5%E7%88%AC%E8%99%AB/" rel="prev" title="动态网页爬虫">
                  <i class="fa fa-chevron-left"></i> 动态网页爬虫
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/11/25/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%8C%96%E6%8E%98-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%89%E5%89%91%E5%AE%A2/" rel="next" title="数据分析与挖掘(数据分析三剑客)">
                  数据分析与挖掘(数据分析三剑客) <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="lv-container" data-id="city" data-uid="MTAyMC81MzYzOC8zMDExMQ=="></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">唧唧歪歪</span>
</div>
<div class="busuanzi-count">
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div> -->

    </div>
  </footer>

  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdn.jsdelivr.net/npm/ribbon.js@1.0.2/dist/ribbon.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  



  <script class="next-config" data-name="nprogress" type="application/json">{"enable":true,"spinner":true}</script>
  <script src="/js/third-party/nprogress.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




<script src="/js/third-party/comments/livere.js"></script>

</body>
</html>
